{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPwSsj80X2sJosyhlrnoYD0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shamantechnology/med-coder-llm/blob/master/MedCoderAI_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set these weaviate URL and API for cloud version in private keys**\n",
        "\n",
        "WEAVIATE_API_KEY\n",
        "WEAVIATE_CLUSTER_URL"
      ],
      "metadata": {
        "id": "Qw1QjT-C8tt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install dependences\n"
      ],
      "metadata": {
        "id": "11jLgkJiqO88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain trulens_eval weaviate-client litellm google-generativeai google-cloud-aiplatform sentence_transformers numpy"
      ],
      "metadata": {
        "id": "fJNxPglXovZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MedCoderAI class with chat input"
      ],
      "metadata": {
        "id": "m1vVB9GanzaE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"oscurodesigns-1279\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}\n",
        "! gcloud auth application-default login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PwgEoFFnkyl"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "MedCoderAI class\n",
        "Interface to RAG based AI\n",
        "\"\"\"\n",
        "import os\n",
        "import subprocess\n",
        "from concurrent.futures import as_completed\n",
        "import weaviate\n",
        "from weaviate.embedded import EmbeddedOptions\n",
        "\n",
        "from trulens_eval import Feedback, LiteLLM, Tru, TruChain, Huggingface\n",
        "\n",
        "from langchain.chat_models import ChatVertexAI\n",
        "from langchain.vectorstores import Weaviate\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import (\n",
        "    ConversationBufferMemory,\n",
        "    ConversationSummaryMemory\n",
        ")\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import (\n",
        "    PromptTemplate,\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "class MedCoderAI:\n",
        "    def __init__(self):\n",
        "        self.cpt_csv = \"./data/2024_DHS_Code_List_Addendum_11_29_2023.csv\"\n",
        "        self.icd_csv = \"./data/Section111ValidICD10-Jan2024.csv\"\n",
        "        self.code_docs = []\n",
        "        self.client = None\n",
        "        self.vectorstore = None\n",
        "        self.llm = ChatVertexAI(\n",
        "            temperature=0.1\n",
        "        )\n",
        "        self.llm_chain = None\n",
        "        self.conversation = None\n",
        "        self.memory = None\n",
        "        self.chain_recorder = None\n",
        "\n",
        "        # generate docs\n",
        "        self.generate_cpt_icd_docs()\n",
        "\n",
        "    def generate_cpt_icd_docs(self):\n",
        "        \"\"\"\n",
        "        Generate langchain docs from CSVs\n",
        "        \"\"\"\n",
        "        print(\"Generating CPT/ICD to langchain docs...\")\n",
        "        try:\n",
        "            cpt_loader = CSVLoader(self.cpt_csv)\n",
        "            self.code_docs += cpt_loader.load()\n",
        "\n",
        "            icd_loader = CSVLoader(self.icd_csv)\n",
        "            self.code_docs += icd_loader.load()\n",
        "        except Exception as err:\n",
        "            print(f\"Error when loading CPT/ICD data: {err}\")\n",
        "\n",
        "    def refresh_token(self) -> str:\n",
        "        result = subprocess.run([\"gcloud\", \"auth\", \"print-access-token\"], capture_output=True, text=True)\n",
        "        if result.returncode != 0:\n",
        "            print(f\"Error refreshing token: {result.stderr}\")\n",
        "            return None\n",
        "        return result.stdout.strip()\n",
        "\n",
        "    def re_instantiate_weaviate(self) -> weaviate.Client:\n",
        "        try:\n",
        "            token = self.refresh_token()\n",
        "\n",
        "            if token:\n",
        "                self.client = weaviate.Client(\n",
        "                    additional_headers={\n",
        "                        \"X-Palm-Api-Key\": token\n",
        "                    },\n",
        "                    embedded_options=EmbeddedOptions(\n",
        "                        additional_env_vars={\n",
        "                            \"ENABLE_MODULES\": \"text2vec-palm\"\n",
        "                        }\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError\n",
        "        except Exception:\n",
        "            raise\n",
        "\n",
        "    def init_conversation(self):\n",
        "        print(\"Initilizing conversations and vectorstores\")\n",
        "        try:\n",
        "            # start weaviate with schemas\n",
        "            # comment out if using cloud\n",
        "            self.re_instantiate_weaviate()\n",
        "\n",
        "            # uncomment if using cloud and set URL and API secrets\n",
        "            # self.client = weaviate.Client(\n",
        "            #     url=userdata.get(\"WEAVIATE_CLUSTER_URL\"),\n",
        "            #     auth_client_secret=weaviate.AuthApiKey(\n",
        "            #         api_key=userdata.get(\"WEAVIATE_API_KEY\"))\n",
        "            # )\n",
        "        except Exception as err:\n",
        "            print(f\"failed to start weviate client: {err}\")\n",
        "            raise\n",
        "\n",
        "        print(f\"Adding {len(self.code_docs)} documents to vectorstore\")\n",
        "        try:\n",
        "            # setup vectorstore and retriever\n",
        "            self.vectorstore = Weaviate.from_documents(\n",
        "                client=self.client,\n",
        "                documents=self.code_docs,\n",
        "                embedding=HuggingFaceEmbeddings(),\n",
        "                by_text=False\n",
        "            )\n",
        "        except Exception as err:\n",
        "            print(f\"failed to add docs to weaviate: {err}\")\n",
        "            raise\n",
        "\n",
        "        template = \"\"\"\n",
        "        You are Betsy who is a professional medical coder.\n",
        "        Answer the users question and return the proper ICD and/or CPT codes or a list of possible ICD and/or CPT codes that one could use for the question.\n",
        "        If you cannot find the answer from the pieces of context, ask the user for more details.\n",
        "\n",
        "        Question: {question}\n",
        "        -------------------------------\n",
        "        Context: {context}\n",
        "        -------------------------------\n",
        "        Chat History: {chat_history}\n",
        "        -------------------------------\"\"\"\n",
        "\n",
        "        qa_prompt = PromptTemplate(\n",
        "            input_variables=[\"context\", \"chat_history\", \"question\"],\n",
        "            template=template\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            print(\"Creating conversational buffer memory\")\n",
        "            # setup memory and convo\n",
        "            self.memory = ConversationBufferMemory(\n",
        "                memory_key=\"chat_history\",\n",
        "                output_key=\"answer\",\n",
        "                return_messages=True\n",
        "            )\n",
        "        except Exception as err:\n",
        "            print(f\"Creating conversational buffer memory failed: {err}\")\n",
        "            raise\n",
        "\n",
        "        try:\n",
        "            print(\"Creating conversational RAG\")\n",
        "            self.conversation = ConversationalRetrievalChain.from_llm(\n",
        "                llm=self.llm,\n",
        "                retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 10}),\n",
        "                memory=self.memory,\n",
        "                verbose=True,\n",
        "                combine_docs_chain_kwargs={\"prompt\": qa_prompt}\n",
        "            )\n",
        "        except Exception as err:\n",
        "            print(f\"Creating conversational RAG failed: {err}\")\n",
        "            raise\n",
        "\n",
        "    def run(self):\n",
        "        # setup conversation\n",
        "        self.init_conversation()\n",
        "\n",
        "        # setup trulens\n",
        "        tru = Tru()\n",
        "        feedbacks = []\n",
        "\n",
        "        litellm_provider = LiteLLM(model_engine=\"chat-bison\")\n",
        "        feedbacks.append(Feedback(litellm_provider.conciseness).on_output())\n",
        "\n",
        "        hugs = Huggingface()\n",
        "        # personal identifying information check\n",
        "        # This is good for names but need to find model for detecting PHI\n",
        "        feedbacks.append(Feedback(hugs.pii_detection).on_input())\n",
        "\n",
        "        self.chain_recorder = TruChain(\n",
        "            self.conversation,\n",
        "            app_id=\"med-coder-llm\",\n",
        "            initial_app_loader=self.init_conversation,\n",
        "            feedbacks=feedbacks\n",
        "        )\n",
        "\n",
        "        # tru.run_dashboard()\n",
        "\n",
        "    def ask_question(self, user_msg) -> str:\n",
        "        rec = None\n",
        "        with self.chain_recorder as recorder:\n",
        "            resp = self.conversation({\"question\": user_msg})\n",
        "            rec = recorder.get()\n",
        "\n",
        "        pii_detected = False\n",
        "        conciseness = 0.0\n",
        "        if rec:\n",
        "            for feedback_future in  as_completed(rec.feedback_results):\n",
        "                feedback, feedback_result = feedback_future.result()\n",
        "\n",
        "                # print(f\"feedback name: {feedback.name}\\n result: {feedback_result.result}\")\n",
        "\n",
        "                # could not get collab to download the needed pii model\n",
        "                # if feedback.name == \"pii_detection\":\n",
        "                #   pii_res = feedback_result\n",
        "                #   if pii_res != None:\n",
        "                #     if pii_res.result() != None:\n",
        "                #       pii_detected = True\n",
        "\n",
        "                if feedback.name == \"conciseness\":\n",
        "                  concisness_res = feedback_result\n",
        "                  if concisness_res != None:\n",
        "                    if concisness_res.result != None:\n",
        "                      conciseness = float(feedback_result.result)\n",
        "\n",
        "        if pii_detected:\n",
        "            return \"I'm sorry but personal information was detected in your question. Please remove any personal information.\"\n",
        "        elif conciseness < 0.5:\n",
        "            return \"Please restate your question in a way the AI can understand and give a better answer\"\n",
        "        else:\n",
        "            return resp[\"answer\"]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from google.cloud import aiplatform\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()\n",
        "    aiplatform.init(project=PROJECT_ID)\n",
        "\n",
        "    md = MedCoderAI()\n",
        "    md.run()\n",
        "\n",
        "    user_msg = \"\"\n",
        "    while True:\n",
        "        user_msg = input(\"Enter in patient description.\\n\")\n",
        "        ai_resp = md.ask_question(user_msg)\n",
        "        print(f\"\\n\\nAI Reponse: {ai_resp}\\n\\n\")"
      ]
    }
  ]
}